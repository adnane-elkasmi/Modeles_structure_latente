{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b38e87",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "# TP 3 Méthodes bayésiennes, MCMC \n",
    "\n",
    "#### Adnane EL KASMI - Alexis Aurissergues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4533e0f5",
   "metadata": {},
   "source": [
    "# Échantillonneur de Gibbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1599e9b",
   "metadata": {},
   "source": [
    "Soit $\\mu_1$ ~ $\\mathcal{N}(\\delta_1,\\frac{1}{\\lambda})$ et $\\mu_2$ ~ $\\mathcal{N}(\\delta_2,\\frac{1}{\\lambda})$\n",
    " \n",
    "\n",
    "On considère la loi de mélange :\n",
    "<center>\n",
    "$\\pi(.|\\theta)$ = $\\sum_{i=1}^2 \\pi_{i}\\phi(.,\\mu_{i},1)$\n",
    "</center>\n",
    "\n",
    "\n",
    "Avec $\\theta$ = ($\\mu_1$, $\\mu_2$).\n",
    " \n",
    "\n",
    "On pose $n_1$ = $\\sum_{i=1}^{n} \\mathbb{1}_{Z_{i} =1} $ et $n_2$ = $ \\sum_{i=1}^{n} \\mathbb{1}_{Z_{i} =2}$, pour i $\\in$ $[1,n]$ ,$Z_{i}$ $\\in$ {$1$,$2$}.\n",
    "\n",
    "\n",
    "<center>\n",
    "    $\\pi(\\mu_1|\\textbf{X})$ $\\propto$ $\\pi(\\mu_1)$$p_{\\mu_1}(\\textbf{X})$ \n",
    "      $\\propto$  $\\pi(\\mu_1)$${\\displaystyle \\prod_{i=1}^{n} f_{\\mu_1}(X_{i})}$  $\\propto$ $e^{\\frac{-\\lambda(\\mu_1 - \\delta_1)^2}{2}}$$e^{\\sum_{i=1}^{n} \\frac{(X_{i}-\\mu_1)^2}{2}} \\mathbb{1}_{Z_{i} = 1}$ $\\propto$ $e^{\\frac{-\\lambda(\\mu_1^2 -2\\mu_1\\delta_1 + \\delta_1)}{2}}$ $e^{-\\frac{(\\sum_{i=1}^{n_1} X_{i}^2 -2\\mu_1X_{i} + \\mu_1^2)}{2}}$ $\\propto$ $e^{\\mu_1^2(-\\frac{\\lambda}{2} - \\frac{n_1}{2}) + \\mu_1(\\delta_1 +\\bar{X}_{n_1}n_1)}$ $\\propto$ $e^{( \\mu_1 - \\frac{ \\delta_1 \\lambda + \\bar{X}_{n_1}}{\\lambda + n_1})(\\frac{\\lambda + n_1}{2})}$\n",
    "\n",
    "</center>\n",
    "\n",
    "À une constance multiplicative près, la densité de la loi $\\pi (\\mu_1 | \\textbf{X})$ est la densité d'une loi normale $\\mathcal{N}( \\frac{\\delta_1 \\lambda + \\bar{X}_{n_1}}{\\lambda + n_1}, \\frac{1}{\\lambda + n_1})$.\n",
    "\n",
    "De la même manière.\n",
    "\n",
    "\n",
    "<center>\n",
    "    $\\pi(\\mu_2|\\textbf{X})$ $\\propto$ $\\pi(\\mu_2)$$p_{\\mu_2}(\\textbf{X})$ \n",
    "      $\\propto$  $\\pi(\\mu_2)$${\\displaystyle \\prod_{i=1}^{n} f_{\\mu_2}(X_{i})}$  $\\propto$ $e^{\\frac{-\\lambda(\\mu_2 - \\delta_2)^2}{2}}$$e^{\\sum_{i=1}^{n} \\frac{(X_{i}-\\mu_2)^2}{2}} \\mathbb{1}_{Z_{i} = 1}$ $\\propto$ $e^{\\frac{-\\lambda(\\mu_2^2 -2\\mu_2\\delta_2 + \\delta_2)}{2}}$ $e^{-\\frac{(\\sum_{i=1}^{n_1} X_{i}^2 -2\\mu_2X_{i} + \\mu_2^2)}{2}}$ $\\propto$ $e^{\\mu_2^2(-\\frac{\\lambda}{2} - \\frac{n_1}{2}) + \\mu_2(\\delta_2 +\\bar{X}_{n_1}n_1)}$ $\\propto$ $e^{( \\mu_2 - \\frac{ \\delta_2 \\lambda + \\bar{X}_{n_1}}{\\lambda + n_1})(\\frac{\\lambda + n_1}{2})}$\n",
    "\n",
    "</center>\n",
    "\n",
    "À une constance multiplicative près, la densité de la loi $\\pi (\\mu_2 | \\textbf{X})$ est la densité d'une loi normale $\\mathcal{N}( \\frac{\\delta_2 \\lambda + \\bar{X}_{n_2}}{\\lambda + n_2}, \\frac{1}{\\lambda + n_2})$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "On initialise l'algorithme avec $\\theta^{0}$ = ($\\mu_1^{0}$,$\\mu_2^{0}$).\n",
    "\n",
    "Pour l'itération t, on note la valeur de $\\theta^{(t)}$:\n",
    "\n",
    "Rappellons que \n",
    "$\\mathbb{P}( Z_{i}^{(t)} = k| \\textbf{X} , \\theta^{(t)})$ = $\\frac{\\pi_{k}^{(t)} \\phi(x_i,\\mu_{k}^{(t)},1)}{ \\sum_{=1}^2  }$\n",
    "\n",
    "$\\pi_k^{(t)} \\phi_{x_i},\\mu_{k}^{(t)}$\n",
    "\n",
    "Initialisation : démarrer avec une valeur arbitraire $\\mathbf{x}^{(0)}=\\left(x_{1}^{(0)}, \\ldots, x_{p}^{(0)}\\right)$. Iteration $t$ : avec $\\left(x_{1}^{(t-1)}, \\ldots, x_{p}^{(t-1)}\\right)$, generer\n",
    "1. $x_{1}^{(t)}$ according to $\\pi_{1}\\left(x_{1} \\mid x_{2}^{(t-1)}, \\ldots, x_{p}^{(t-1)}\\right)$,\n",
    "2. $x_{2}^{(t)}$ according to $\\pi_{2}\\left(x_{2} \\mid x_{1}^{(t)}, x_{3}^{(t-1)}, \\ldots, x_{p}^{(t-1)}\\right)$,\n",
    "3. $x_{p}^{(t)}$ according to $\\pi_{p}\\left(x_{p} \\mid x_{1}^{(t)}, \\ldots, x_{p-1}^{(t)}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c091d8",
   "metadata": {},
   "source": [
    "## Implémentation sur R de l’algorithme de l’échantillonneur de Gibbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c33f5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gibbs_Sampling <- function(X,initial,prior,iter) {\n",
    "    \n",
    "    \n",
    "\n",
    "# (Initialisation)\n",
    "n <- length(X)\n",
    "mu1 <- initial$mu1\n",
    "mu2 <- initial$mu2\n",
    "pi <- initial$pi\n",
    "delta1 <- prior$delta1\n",
    "delta2 <- prior$delta2\n",
    "lambda <- prior$lambda\n",
    "\n",
    "loglik <- rep(NA, iter)\n",
    "vec_mu1 <- rep(NA, iter)\n",
    "vec_mu2 <- rep(NA, iter)\n",
    "\n",
    "for(t in 1:(iter)){\n",
    "\n",
    "# (Simulation des variables latentes z_i sachant X et mu)\n",
    "P <- matrix(NA, length(X), 2)\n",
    "P[,1]<-pi*dnorm(X,mu1,1)/(pi*dnorm(X,mu1,1)+(1-pi)*dnorm(X,mu2,1))\n",
    "P[,2]<-(1-pi)*dnorm(X,mu2,1)/(pi*dnorm(X,mu1,1)+(1-pi)*dnorm(X,mu2,1))\n",
    "Z <- apply(P,1,function(row)sample(1:2, size=1, prob=row))\n",
    "\n",
    "# (Générer nouveaux candidats)\n",
    "n1 <- sum(Z==1)\n",
    "n2 <- sum(Z==2)\n",
    "\n",
    "s1 <- sum(X[Z==1])\n",
    "s2 <- sum(X[Z==2])\n",
    "\n",
    "mu1.new <- rnorm(1, (lambda*delta1+s1)/(lambda+n1), 1/(lambda+n1))\n",
    "mu2.new <- rnorm(1, (lambda*delta2+s2)/(lambda+n2), 1/(lambda+n2))\n",
    "\n",
    "# (Mise à jour)\n",
    "mu1 <- mu1.new\n",
    "mu2 <- mu2.new\n",
    "\n",
    "vec_mu1[t] <- mu1\n",
    "vec_mu2[t] <- mu2\n",
    "\n",
    "loglik[t] <- logvrais(X,list(pi=c(pi,1-pi), mu=c(mu1,mu2), sigma=c(1,1)))\n",
    "}\n",
    "\n",
    "res <- list(mu1=vec_mu1, mu2=vec_mu2, loglik=loglik)\n",
    "return (res)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a74ca10",
   "metadata": {},
   "source": [
    "# Algorithme de Metropolis-Hastings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b1d75",
   "metadata": {},
   "source": [
    "Comme indiqué dans l'énoncé, on prend comme loi de proposition $Q$ une loi normale qu'on centre en la valeur courante des paramètres, avec $\\zeta$ comme écart-type et en simulant des propositions *indépendantes* pour chacun des deux paramètres.\n",
    "\n",
    "On choisit un nombre d'itérations $T>0$ et on initialise : $(\\mu_1^{(0)}, \\mu_2^{(0)})$.\n",
    "\n",
    "Pour $t$ allant de $1$ à $T$ :\n",
    "\n",
    "1) On simule des variables aléatoires $y_1$ et $y_2$ suivant respectivement les lois $\\mathcal{N}(\\mu_1^{(t-1)},\\zeta^2)$ et  $\\mathcal{N}(\\mu_2^{(t-1)},\\zeta^2)$.\n",
    "\n",
    "2) On calcule $ r(\\mu_k^{(t-1)}, y_k)$ pour $k = 1, 2 $\n",
    "\n",
    "Où $ r(\\mu_k^{(t-1)}, y_k) = \\min(\\displaystyle \\frac{f(y_k) q(\\mu_k^{(t-1)} | y_k)}{f(\\mu_k^{(t-1)}) q(y_k | \\mu_k^{(t-1)})}, 1) $, avec q la densité de la loi de proposition\n",
    "      \n",
    "3) Et on pose :\n",
    "\n",
    "$$\n",
    "\\mu_k^{(t)} = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        y_k & \\mbox{avec probabilité $r( \\mu_k^{(t-1)},y_k)$ }   \\\\\n",
    "        \\mu_k^{(t-1)} & \\mbox{avec probabilité  $1-r( \\mu_k^{(t-1)},y_k)$}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "(pour $k = 1, 2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e7fbf1",
   "metadata": {},
   "source": [
    "## Implémentation sur R de l'algorithme de Metropolis-Hastings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "790f7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (on crée une fonction pour la densité du mélange)\n",
    "\n",
    "f = function(x, m1, m2, p) {\n",
    "  return(p*dnorm(x, mean = m1, sd = 1)+(1-p)*dnorm(x, mean = m2, sd = 1))\n",
    "}\n",
    "\n",
    "Metropolis_Hastings = function(X, initial, prior, zeta, iter) {\n",
    "  \n",
    "  # (initialisation)\n",
    "  n = length(X)\n",
    "  mu1 = initial$mu1\n",
    "  mu2 = initial$mu2\n",
    "  pi = initial$pi\n",
    "  delta1 = prior$delta1\n",
    "  delta2 = prior$delta2\n",
    "  lambda = prior$lambda\n",
    "  loglik = rep(NA, iter)\n",
    "  vec_mu1 = rep(NA, iter)\n",
    "  vec_mu2 = rep(NA, iter)\n",
    "  \n",
    "  # (Itérations)\n",
    "  for(t in 1:iter) {\n",
    "      \n",
    "    y1 = rnorm(mean = mu1, sd = zeta)\n",
    "    y2 = rnorm(mean = mu2, sd = zeta)\n",
    "    p1 = prod(f(X, mu1, mu2, pi)/f(X, y1, y2, pi))\n",
    "      \n",
    "    # (Taux d'acceptation)  \n",
    "    rho1 = min(1, p*dnorm(x = y1, mean = delta1, sd = 1/lambda)/dnorm(x = mu1, mean = delta1, sd = 1/lambda))\n",
    "    rho2 = min(1, p*dnorm(x = y2, mean = delta2, sd = 1/lambda)/dnorm(x = mu2, mean = delta2, sd = 1/lambda))\n",
    "    \n",
    "    \n",
    "    u1 = runif(min = 0, max = 1)\n",
    "    u2 = runif(min = 0, max = 1)\n",
    "      \n",
    "    if(u1 < rho1) {\n",
    "      mu1 = y1\n",
    "    }\n",
    "    if(u2 < rho2){\n",
    "      mu2 = y2\n",
    "    }\n",
    "    vec_mu1[t] = mu1\n",
    "    vec_mu2[t] = mu2\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a1f80",
   "metadata": {},
   "source": [
    "# Comparaison des algorithmes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39146af2",
   "metadata": {},
   "source": [
    "Honnêtement, on n'avait pas beaucoup de temps pour traiter cette partie, mais on a bien compris le pricipe. merci de votre compréhension."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
